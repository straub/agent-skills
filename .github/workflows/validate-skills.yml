name: Validate Skills

on:
  push:
    branches: [ main ]
    paths:
      - 'skills/**'
      - 'promptfooconfig.yaml'
      - 'package.json'
      - '.github/workflows/validate-skills.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'skills/**'
      - 'promptfooconfig.yaml'
      - 'package.json'
      - '.github/workflows/validate-skills.yml'
  workflow_dispatch:

jobs:
  validate:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: false
      
      - name: Clone skills-ref tool
        run: |
          git clone --depth 1 https://github.com/agentskills/agentskills.git /tmp/agentskills
      
      - name: Install skills-ref
        run: |
          cd /tmp/agentskills/skills-ref
          uv sync
      
      - name: Validate skills
        run: |
          # Activate the virtual environment
          source /tmp/agentskills/skills-ref/.venv/bin/activate
          
          # Find all skill directories (directories containing SKILL.md)
          skill_dirs=$(find skills -type f -name "SKILL.md" -exec dirname {} \;)
          
          if [ -z "$skill_dirs" ]; then
            echo "No skills found to validate."
            exit 0
          fi
          
          # Validate each skill
          exit_code=0
          for skill_dir in $skill_dirs; do
            echo "Validating: $skill_dir"
            if skills-ref validate "$skill_dir"; then
              echo "‚úì $skill_dir is valid"
            else
              echo "‚úó $skill_dir validation failed"
              exit_code=1
            fi
          done
          
          exit $exit_code

  evaluate:
    runs-on: ubuntu-latest
    needs: validate
    permissions:
      contents: read
      models: read
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      
      - name: Install dependencies
        run: npm ci
      
      - name: Run Promptfoo evaluations
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          PROMPTFOO_DISABLE_TELEMETRY: 1
        run: |
          # GitHub Models requires GITHUB_TOKEN
          if [ -z "$GITHUB_TOKEN" ]; then
            echo "‚ö†Ô∏è  No GITHUB_TOKEN found"
            echo "‚ö†Ô∏è  Skipping LLM-based evaluations"
            echo "‚ÑπÔ∏è  GitHub Models evaluations require GITHUB_TOKEN (automatically available in GitHub Actions)"
            exit 0
          fi
          
          # Run evaluations with GitHub Models (OpenAI and Claude)
          echo "üîç Running evaluations with GitHub Models (OpenAI GPT-4o-mini and Claude 4 Sonnet)"
          npx promptfoo eval --no-table
          
          # Check for failures in the results
          if [ -f promptfoo-results.json ]; then
            echo ""
            echo "üìä Evaluation Summary:"
            # Extract pass/fail stats from the JSON results
            grep -o '"passed":[0-9]*' promptfoo-results.json | head -1 || echo "Results file found"
            
            # Note: Promptfoo exits with non-zero if there are failures
            # The job will fail automatically if the eval command fails
          fi
      
      - name: Check evaluation results
        if: always()
        run: |
          if [ -f promptfoo-results.json ]; then
            echo "‚úÖ Evaluation completed. Results saved."
            # Show summary if jq is available
            if command -v jq &> /dev/null; then
              echo ""
              echo "Summary:"
              jq -r '.stats | "Total: \(.totalTests) | Passed: \(.passed) | Failed: \(.failed) | Errors: \(.errors)"' promptfoo-results.json || true
            fi
          else
            echo "‚ÑπÔ∏è  No evaluation results found (likely skipped due to missing API keys)"
          fi
      
      - name: Upload evaluation results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: promptfoo-results
          path: promptfoo-results*.json
          retention-days: 30
