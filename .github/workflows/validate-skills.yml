name: Validate Skills

on:
  push:
    branches: [ main ]
    paths:
      - 'skills/**'
      - 'promptfooconfig.yaml'
      - 'package.json'
      - '.github/workflows/validate-skills.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'skills/**'
      - 'promptfooconfig.yaml'
      - 'package.json'
      - '.github/workflows/validate-skills.yml'
  workflow_dispatch:

jobs:
  validate:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: false
      
      - name: Clone skills-ref tool
        run: |
          git clone --depth 1 https://github.com/agentskills/agentskills.git /tmp/agentskills
      
      - name: Install skills-ref
        run: |
          cd /tmp/agentskills/skills-ref
          uv sync
      
      - name: Validate skills
        run: |
          # Activate the virtual environment
          source /tmp/agentskills/skills-ref/.venv/bin/activate
          
          # Find all skill directories (directories containing SKILL.md)
          skill_dirs=$(find skills -type f -name "SKILL.md" -exec dirname {} \;)
          
          if [ -z "$skill_dirs" ]; then
            echo "No skills found to validate."
            exit 0
          fi
          
          # Validate each skill
          exit_code=0
          for skill_dir in $skill_dirs; do
            echo "Validating: $skill_dir"
            if skills-ref validate "$skill_dir"; then
              echo "‚úì $skill_dir is valid"
            else
              echo "‚úó $skill_dir validation failed"
              exit_code=1
            fi
          done
          
          exit $exit_code

  evaluate:
    runs-on: ubuntu-latest
    needs: validate
    permissions:
      contents: read
      models: read
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      
      - name: Install dependencies
        run: npm ci
      
      - name: Run Promptfoo evaluations
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          PROMPTFOO_DISABLE_TELEMETRY: 1
        run: |
          # Check if any API key is available
          if [ -z "$OPENAI_API_KEY" ] && [ -z "$ANTHROPIC_API_KEY" ] && [ -z "$GITHUB_TOKEN" ]; then
            echo "‚ö†Ô∏è  No API keys found (OPENAI_API_KEY, ANTHROPIC_API_KEY, or GITHUB_TOKEN)"
            echo "‚ö†Ô∏è  Skipping LLM-based evaluations"
            echo "‚ÑπÔ∏è  To enable full evaluations, add API keys to repository secrets"
            exit 0
          fi
          
          # Build provider list based on available keys
          providers=""
          
          if [ -n "$OPENAI_API_KEY" ]; then
            echo "‚úì OpenAI API key found"
            providers="$providers -p openai:gpt-4o-mini"
          fi
          
          if [ -n "$ANTHROPIC_API_KEY" ]; then
            echo "‚úì Anthropic API key found"
            providers="$providers -p anthropic:claude-3-5-sonnet-20241022"
          fi
          
          if [ -n "$GITHUB_TOKEN" ]; then
            echo "‚úì GitHub token found"
            providers="$providers -p github:gpt-4o-mini"
          fi
          
          # Run evaluations with all available providers
          echo "üîç Running evaluations with providers:$providers"
          npx promptfoo eval $providers --no-table
          
          # Check for failures in the results
          if [ -f promptfoo-results.json ]; then
            echo ""
            echo "üìä Evaluation Summary:"
            # Extract pass/fail stats from the JSON results
            grep -o '"passed":[0-9]*' promptfoo-results.json | head -1 || echo "Results file found"
            
            # Note: Promptfoo exits with non-zero if there are failures
            # The job will fail automatically if the eval command fails
          fi
      
      - name: Check evaluation results
        if: always()
        run: |
          if [ -f promptfoo-results.json ]; then
            echo "‚úÖ Evaluation completed. Results saved."
            # Show summary if jq is available
            if command -v jq &> /dev/null; then
              echo ""
              echo "Summary:"
              jq -r '.stats | "Total: \(.totalTests) | Passed: \(.passed) | Failed: \(.failed) | Errors: \(.errors)"' promptfoo-results.json || true
            fi
          else
            echo "‚ÑπÔ∏è  No evaluation results found (likely skipped due to missing API keys)"
          fi
      
      - name: Upload evaluation results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: promptfoo-results
          path: promptfoo-results*.json
          retention-days: 30
